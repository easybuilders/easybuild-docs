---
search:
  boost: 0.5
---
# llama.cpp

Inference of Meta's LLaMA model (and others) in pure C/C++

*homepage*: <https://github.com/ggerganov/llama.cpp>

version | versionsuffix | toolchain
--------|---------------|----------
``b4595`` | ``-CUDA-12.1.1`` | ``foss/2023a``
``b4595`` |  | ``foss/2023a``


*(quick links: [(all)](../index.md) - [0](../0/index.md) - [a](../a/index.md) - [b](../b/index.md) - [c](../c/index.md) - [d](../d/index.md) - [e](../e/index.md) - [f](../f/index.md) - [g](../g/index.md) - [h](../h/index.md) - [i](../i/index.md) - [j](../j/index.md) - [k](../k/index.md) - [l](../l/index.md) - [m](../m/index.md) - [n](../n/index.md) - [o](../o/index.md) - [p](../p/index.md) - [q](../q/index.md) - [r](../r/index.md) - [s](../s/index.md) - [t](../t/index.md) - [u](../u/index.md) - [v](../v/index.md) - [w](../w/index.md) - [x](../x/index.md) - [y](../y/index.md) - [z](../z/index.md))*

